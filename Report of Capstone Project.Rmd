---
title: "Capstone Project II"
author: "Tarasov Oleksiy"
date: "6/2/2019"
output: html_document
---

# Introduction
This report is part of the HarvardX Certification of the edX HarvardX: PH125.9x Data Science: Capstone Online Training
The data was taken in acccordance with the proposal of the course from Kaggle curated list of datasets
link https://www.kaggle.com/annavictoria/ml-friendly-public-datasets?utm_medium=email&utm_source=intercom&utm_campaign=data+projects+onboarding
The following dataset were chosen for the analysis

[<Adult Census Income>](https://www.kaggle.com/uciml/adult-census-income) https://www.kaggle.com/uciml/adult-census-income
in case that connection to the Kaggle will not work  I have copied the file to my github directory 
https://github.com/alexej-tarasov/CapstoneII file adult.csv

# Overview
The target cell is the column which predicts whether the person has incomes more or less then 50000 USD annually


# Executive Summary


# Methods/analysis 
This section  explains the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and your modeling approach;
First library for analysis should be activated
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(tidyr)
library(plotly)
library(tidyverse)
library(lubridate)
library(broom)
library(ggplot2)
library(RCurl)
library(kableExtra)
library(e1071)
library(parallel)
library(doParallel)
library(rpart)
library(Rborist)
```

In this section data will be read
```{r}
data<-read.csv("~/CapstoneII/CapstoneII/adult.csv")
#library(httr)
#nuts <-read.csv(text=GET("https://raw.githubusercontent.com/alexej-tarasov/CapstoneII/adult.csv"), header=T)
#x <- read.csv("https://github.com/alexej-tarasov/CapstoneII/adult.csv")
#data2 <- read.csv(text = x)
```
The data has following attributes

**Target income**: >50K, <=50K

**age**: continuous

**workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked

**fnlwgt**: continuous

**education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool

**education-num**: continuous

**marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse

**occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces

**relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried

**race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black

**sex**: Female, Male

**capital-gain**: continuous

**capital-loss**: continuous

**hours-per-week**: continuous

**native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands


Below the example of the top10 rows is shown
```{r}
as_tibble(head(data,n=10))%>%kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


The data is separated to the test and validation set using the caret package.
Validation set will be 10 % percent of the total records in adult.csv
```{r}
set.seed(1)
test_index <- createDataPartition(y = data$income, times = 1, p = 0.1, list = FALSE)
trainset <- data[-test_index,]
temp <- data[test_index,]
validation <- temp %>% 
     semi_join(trainset, by = "workclass") %>%
     semi_join(trainset, by = "education") %>%
     semi_join(trainset, by = "marital.status") %>%
     semi_join(trainset, by = "occupation") %>%
     semi_join(trainset, by = "relationship")%>%
     semi_join(trainset, by = "native.country")
accuracy_results<-data.frame()
```

```{r}
print(paste("Trainset has ",count(trainset)," records"))
print(paste("Validation set has ",count(validation)," records"))
```



Check the importance of the variables
```{r}
model1<-rpart(income~.,data = trainset)
var1<-varImp(model1) # Calculation of iimportance
var1<-rownames_to_column(var1) # Transfer of the row names to the separate column in data frame
varImpotance<-var1[order(-var1$Overall),] # Reorder of the importances by value
ggplot(varImpotance,aes(x = reorder(rowname, Overall),y=Overall),fill = variable)+ #chart creation
  geom_bar(stat = "identity",fill="black")+
  xlab("Features")+ylab("Overall importance")+ggtitle("Features importance in dataset")+
  theme_minimal()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+coord_flip()
```
Features workclass,sex, race, native.country,hours.per.week,age and fnlwgt has no significant importance to the prediction in and these features will be excluded from the analysis
```{r}
trainset_opt<-trainset%>%select(-workclass,-sex, -race, -native.country,-fnlwgt,-hours.per.week,-age)
as_tibble(trainset_opt)
```

Below first try is to use random forest algoriithm with parallel processing with parameter selection
Duration of theh process with 7 parallel cores is ca 10 minutes
```{r}
tsCVstart<-Sys.time()
print(paste("Process started",tsCVstart))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)  # registration of the clusters for parallel processing
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE) # adjusting control for allowing parrallel processing
y<-train(income~., method="rf",data = trainset_opt,trControl = fitControl) # creation of the model for all attributes, take a lot of time 
y # display of the model
ggplot(y,highlight = T)
prediction_rf<-predict(y,validation)
confusionMatrix(data=prediction_rf,reference=validation$income)
stopCluster(cluster)
registerDoSEQ()
rf_accuracy<-confusionMatrix(data=prediction_rf,reference=validation$income)$overall[["Accuracy"]]

accuracy_results <- bind_rows(accuracy_results,
                          data_frame(method="rf",  
                                     Accuracy = rf_accuracy))
print(accuracy_results)
tsCVfinish<-Sys.time()
print(paste("Process finished ",tsCVfinish))
print(paste("Process duration ",trunc(unclass(tsCVfinish)-unclass(tsCVstart)),"sec"))
```

Training model with rborist
```{r}
tsCVstart<-Sys.time()
print(paste("Process started",tsCVstart))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)  # registration of the clusters for parallel processing
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE) # adjusting control for allowing parrallel processing
y<-train(income~., method="xgbDART",data = trainset_opt,trControl = fitControl) # creation of the model for all attributes, take a lot of time 
y # display of the model
#ggplot(y,highlight = T)
prediction_rf<-predict(y,validation)
confusionMatrix(data=prediction_rf,reference=validation$income)
stopCluster(cluster)
registerDoSEQ()
rf_accuracy<-confusionMatrix(data=prediction_rf,reference=validation$income)$overall[["Accuracy"]]

accuracy_results <- bind_rows(accuracy_results,
                          data_frame(method="xgBoost",  
                                     Accuracy = rf_accuracy))
print(accuracy_results)
tsCVfinish<-Sys.time()
print(paste("Process finished ",tsCVfinish))
print(paste("Process duration ",trunc(unclass(tsCVfinish)-unclass(tsCVstart)),"sec"))
```
Training model with gbm
```{r}
tsCVstart<-Sys.time()
print(paste("Process started",tsCVstart))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)  # registration of the clusters for parallel processing
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE) # adjusting control for allowing parrallel processing
y<-train(income~., method="gbm",data = trainset_opt,trControl = fitControl) # creation of the model for all attributes, take a lot of time 
y # display of the model
#ggplot(y,highlight = T)
prediction_rf<-predict(y,validation)
confusionMatrix(data=prediction_rf,reference=validation$income)
stopCluster(cluster)
registerDoSEQ()
rf_accuracy<-confusionMatrix(data=prediction_rf,reference=validation$income)$overall[["Accuracy"]]

accuracy_results <- bind_rows(accuracy_results,
                          data_frame(method="GBM ",  
                                     Accuracy = rf_accuracy))
print(accuracy_results)
tsCVfinish<-Sys.time()
print(paste("Process finished ",tsCVfinish))
print(paste("Process duration ",trunc(unclass(tsCVfinish)-unclass(tsCVstart)),"sec"))
```
Training model with lvq
```{r}
tsCVstart<-Sys.time()
print(paste("Process started",tsCVstart))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)  # registration of the clusters for parallel processing
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE) # adjusting control for allowing parrallel processing
y<-train(income~., method="lvq",data = trainset_opt,trControl = fitControl) # creation of the model for all attributes, take a lot of time 
y # display of the model
#ggplot(y,highlight = T)
prediction_rf<-predict(y,validation)
confusionMatrix(data=prediction_rf,reference=validation$income)
stopCluster(cluster)
registerDoSEQ()
rf_accuracy<-confusionMatrix(data=prediction_rf,reference=validation$income)$overall[["Accuracy"]]

accuracy_results <- bind_rows(accuracy_results,
                          data_frame(method="lvq",  
                                     Accuracy = rf_accuracy))
print(accuracy_results)
tsCVfinish<-Sys.time()
print(paste("Process finished ",tsCVfinish))
print(paste("Process duration ",trunc(unclass(tsCVfinish)-unclass(tsCVstart)),"sec"))
```
# Results section
After evaluation of the methods rf, rborist, lvq and GBM, the best model was selected. 
The best model is...
The selection was made based on the Accuracy
# Conclusion section.
